version: '3.8'

services:
  triton-server:
    image: nvcr.io/nvidia/tritonserver:23.10-py3
    container_name: triton-mistral
    ports:
      - "8000:8000"  # HTTP
      - "8001:8001"  # gRPC
      - "8002:8002"  # Metrics
    volumes:
      - ../model_repository:/models
      - ./triton-requirements.txt:/tmp/triton-requirements.txt
    command: >
      bash -c "
        pip install -r /tmp/triton-requirements.txt &&
        tritonserver --model-repository=/models
          --backend-config=python,shm-default-byte-size=134217728
          --log-verbose=1
          --strict-model-config=false
          --exit-on-error=false
      "
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s